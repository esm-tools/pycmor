general:
  name: "AWI-ESM-1-1-lr PI Control"
  description: "CMOR configuration for AWIESM 1.1 LR"
  maintainer: "pgierz"
  email: "pgierz@awi.de"
  cmor_version: "CMIP6"
  mip: "CMIP"
  CMIP_Tables_Dir: "/work/ab0246/a270077/SciComp/Projects/pymorize/cmip6-cmor-tables/Tables" 
pymorize:
  # parallel: True
  warn_on_no_rule: False
  use_flox: True
  cluster_mode: fixed
  fixed_jobs: 12
  # minimum_jobs: 8
  # maximum_jobs: 30
  dimensionless_mapping_table: ../data/dimensionless_mappings.yaml
rules:
  - name: paul_example_rule
    description: "You can put some text here"
    inputs:
      - path: /work/ba1103/a270073/out/awicm-1.0-recom/awi-esm-1-1-lr_kh800/piControl/outdata/fesom
        pattern: CO2f_fesom_.*nc
    cmor_variable: fgco2
    model_variable: CO2f
    output_directory: .
    variant_label: r1i1p1f1
    experiment_id: piControl
    #source_id: ocean
    source_id: AWI-CM-1-1-HR
    model_component: seaIce
    pipelines:
      - default
  - name: Salinity of the Ocean
    description: "Here is some text for humans. It isn't used anywhere."
    inputs:
      - path: /work/ba1103/a270073/out/awicm-1.0-recom/awi-esm-1-1-lr_kh800/piControl/outdata/fesom
        pattern: so_fesom_.*nc
    cmor_variable: so
    model_variable: so
    output_directory: .
    variant_label: r1i1p1f1
    experiment_id: piControl
    #source_id: ocean
    source_id: AWI-CM-1-1-HR
    model_component: seaIce
    pipelines:
      - default
  - name: paul_example_rule
    description: "You can put some text here"
    inputs:
      - path: /work/ba1103/a270073/out/awicm-1.0-recom/awi-esm-1-1-lr_kh800/piControl/outdata/fesom
        pattern: thetao_fesom_.*nc
    cmor_variable: thetao
    model_variable: thetao
    output_directory: .
    variant_label: r1i1p1f1
    experiment_id: piControl
    #source_id: ocean
    source_id: AWI-CM-1-1-HR
    model_component: ocean
    pipelines:
      - default
pipelines:
  - name: default
    steps:
      - "pymorize.gather_inputs.load_mfdataset"
      - "pymorize.generic.get_variable"
      - "pymorize.timeaverage.compute_average"
      - "pymorize.units.handle_unit_conversion"
      - "pymorize.generic.trigger_compute"
      - "pymorize.generic.show_data"
      - "pymorize.global_attributes.set_global_attributes"
      - "pymorize.files.save_dataset"

# Settings for using dask-distributed
distributed:
  worker:
    memory:
      target: 0.6           # Target 60% of worker memory usage
      spill: 0.7            # Spill to disk when 70% of memory is used
      pause: 0.8            # Pause workers if memory usage exceeds 80%
      terminate: 0.95       # Terminate workers at 95% memory usage
    resources:
      CPU: 4                # Assign 4 CPUs per worker
    death-timeout: 600      # Worker timeout if no heartbeat (seconds): Keep workers alive for 5 minutes

# SLURM-specific settings for launching workers
jobqueue:
  slurm:
    name: pymorize-worker
    queue: compute                # SLURM queue/partition to submit jobs
    account: ab0246               # SLURM project/account name
    cores: 4                      # Number of cores per worker
    memory: 128GB                 # Memory per worker
    walltime: '00:30:00'          # Maximum walltime per job
    interface: ib0                # Network interface for communication
    job-extra-directives:         # Additional SLURM job options
      - '--exclusive'             # Run on exclusive nodes
      - '--nodes=1'

    # Worker template
    worker-extra:
      - "--nthreads"
      - 4
      - "--memory-limit"
      - "128GB"
      - "--lifetime"
      - "25m"
      - "--lifetime-stagger"
      - "4m"


    # How to launch workers and scheduler
    job-cpu: 128
    job-mem: 256GB
    # worker-command: dask-worker
    processes: 4  # Limited by memory per worker!
    # scheduler-command: dask-scheduler
    
    # # How to launch workers and scheduler
    # worker-template:
    #   # Command to launch a Dask worker via SLURM
    #   command: |
    #     srun --ntasks=1 --cpus-per-task=4 --mem=128G dask-worker \
    #       --nthreads 4 --memory-limit 128GB

    # # Command to launch the Dask scheduler
    # scheduler-template:
    #   command: |
    #     srun --ntasks=1 --cpus-per-task=1 dask-scheduler
