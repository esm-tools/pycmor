general:
  name: "AWI-ESM-1-1-lr PI Control"
  description: "CMOR configuration for AWIESM 1.1 LR"
  maintainer: "pgierz"
  email: "pgierz@awi.de"
  cmor_version: "CMIP6"
  mip: "CMIP"
  CMIP_Tables_Dir: "/work/ab0246/a270077/SciComp/Projects/pymorize/cmip6-cmor-tables/Tables"
pymorize:
  warn_on_no_rule: False
  minimum_jobs: 8
  maximum_jobs: 10
rules:
  - name: paul_example_rule
    description: "You can put some text here"
    inputs:
      - path: /work/ba1103/a270073/out/awicm-1.0-recom/awi-esm-1-1-lr_kh800/piControl/outdata/fesom
        pattern: CO2f_fesom_.*nc
    cmor_variable: fgco2
    model_variable: CO2f
    pipelines: ["devel"]
  - name: Temperature Rule
    inputs:
      - path: /work/ba1103/a270073/out/awicm-1.0-recom/awi-esm-1-1-lr_kh800/piControl/outdata/fesom
        pattern: thetao_fesom_.*nc
    cmor_variable: thetao
    model_variable: thetao
    pipelines: ["devel"]
  - name: Salt Rule
    inputs:
      - path: /work/ba1103/a270073/out/awicm-1.0-recom/awi-esm-1-1-lr_kh800/piControl/outdata/fesom
        pattern: so_fesom_.*nc
    cmor_variable: so
    model_variable: so
    pipelines: ["devel"]
pipelines:
  - name: devel
    steps:
      - "pymorize.gather_inputs.load_mfdataset"
      - "pymorize.generic.get_variable"
      - "pymorize.generic.resample_yearly"
      # - "pymorize.generic.multiyear_monthly_mean"
      # - "pymorize.units.handle_unit_conversion"
      - "pymorize.generic.trigger_compute"
      - "pymorize.generic.show_data"
distributed:
  worker:
    memory:
      target: 0.6           # Target 60% of worker memory usage
      spill: 0.7            # Spill to disk when 70% of memory is used
      pause: 0.8            # Pause workers if memory usage exceeds 80%
      terminate: 0.95       # Terminate workers at 95% memory usage
    resources:
      CPU: 4               # Assign 4 CPUs per worker
    death-timeout: 60       # Worker timeout if no heartbeat (seconds)

# SLURM-specific settings for launching workers
jobqueue:
  slurm:
    queue: compute                # SLURM queue/partition to submit jobs
    project: ab0246               # SLURM project/account name
    cores: 4                      # Number of cores per worker
    memory: 128GB                  # Memory per worker
    walltime: '00:30:00'          # Maximum walltime per job
    interface: ib0                # Network interface for communication
    job-extra:                    # Additional SLURM job options
      - '--exclusive'             # Run on exclusive nodes
    
    # How to launch workers and scheduler
    worker-template:
      # Command to launch a Dask worker via SLURM
      command: |
        srun --ntasks=1 --cpus-per-task=4 --mem=128G dask-worker \
          --nthreads 4 --memory-limit 128GB --death-timeout 60

    # Command to launch the Dask scheduler
    scheduler-template:
      command: |
        srun --ntasks=1 --cpus-per-task=1 dask-scheduler
