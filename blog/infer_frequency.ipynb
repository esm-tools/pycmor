{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Guessing: A Smarter Way to Infer Time Frequencies in Climate Data\n",
    "\n",
    "*A practical demonstration of robust frequency inference for climate time series.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cftime\n",
    "\n",
    "# Import the infer_frequency function from pymor\n",
    "from pymor.core.infer_freq import infer_frequency, FrequencyResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why `xarray.infer_freq` often returns `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**: Non-standard calendar (360_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xarray.infer_freq failed: <class 'cftime._cftime.Datetime360Day'> is not convertible to datetime, at position 0\n",
      "pymor.infer_frequency result: M\n"
     ]
    }
   ],
   "source": [
    "cftime_360day = [\n",
    "    cftime.Datetime360Day(2000, 1, 16),\n",
    "    cftime.Datetime360Day(2000, 2, 16),\n",
    "    cftime.Datetime360Day(2000, 3, 16),\n",
    "    cftime.Datetime360Day(2000, 4, 16),\n",
    "]\n",
    "\n",
    "# pandas.infer_freq does not understand cftime and so xarray.infer_freq does not\n",
    "try:\n",
    "    xr_result = xr.infer_freq(cftime_360day)\n",
    "    print(f\"xarray.infer_freq result: {xr_result}\")\n",
    "except Exception as e:\n",
    "    print(f\"xarray.infer_freq failed: {e}\")\n",
    "\n",
    "# Now, with out robust implementation\n",
    "pymor_result = infer_frequency(cftime_360day)\n",
    "print(f\"pymor.infer_frequency result: {pymor_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Example 2**: Monthly data with missing month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xarray.infer_freq result: None\n",
      "pymor.infer_frequency result: M\n"
     ]
    }
   ],
   "source": [
    "times_with_gap = pd.to_datetime([\"2000-01-31\", \"2000-02-29\", \"2000-04-30\"])  # March is missing\n",
    "\n",
    "# Test xarray's infer_freq\n",
    "xr_result = xr.infer_freq(times_with_gap)\n",
    "print(f\"xarray.infer_freq result: {xr_result}\")\n",
    "\n",
    "# Test our implementation\n",
    "pymor_result = infer_frequency(times_with_gap)\n",
    "print(f\"pymor.infer_frequency result: {pymor_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Example 3**: Unanchored montly data\n",
    "\n",
    "Anchored montly data has date-stamp on either month-start (\"MS\") or month-end (\"ME\").\n",
    "\n",
    "Unanchored monthly data has date-stamp with an offset in date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: month_start\n",
      "Data:\n",
      "[Timestamp('2000-01-01 00:00:00'), Timestamp('2000-02-01 00:00:00'), Timestamp('2000-03-01 00:00:00'), Timestamp('2000-04-01 00:00:00')]\n",
      "Inference:\n",
      "  → xarray.infer_freq : MS\n",
      "  → pymor.infer_frequency : MS\n",
      "--------------------------------------------------\n",
      "Name: month_end\n",
      "Data:\n",
      "[Timestamp('2000-01-31 00:00:00'), Timestamp('2000-02-29 00:00:00'), Timestamp('2000-03-31 00:00:00'), Timestamp('2000-04-30 00:00:00')]\n",
      "Inference:\n",
      "  → xarray.infer_freq : ME\n",
      "  → pymor.infer_frequency : ME\n",
      "--------------------------------------------------\n",
      "Name: unachored_offset\n",
      "Data:\n",
      "[Timestamp('2000-01-06 00:00:00'), Timestamp('2000-02-06 00:00:00'), Timestamp('2000-03-06 00:00:00'), Timestamp('2000-04-06 00:00:00')]\n",
      "Inference:\n",
      "  → xarray.infer_freq : None\n",
      "  → pymor.infer_frequency : M\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"month_start\": pd.date_range(\"2000-01-01\", periods=4, freq=\"MS\"),\n",
    "    \"month_end\": pd.date_range(\"2000-01-01\", periods=4, freq=\"ME\"),\n",
    "    \"unachored_offset\": pd.date_range(\"2000-01-01\", periods=4, freq=\"MS\") + pd.Timedelta(days=5),\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(\"Data:\")\n",
    "    print(list(data))\n",
    "    xr_result = xr.infer_freq(data)\n",
    "    print(\"Inference:\")\n",
    "    print(f\"  → xarray.infer_freq : {xr_result}\")\n",
    "    pymor_result = infer_frequency(data)\n",
    "    print(f\"  → pymor.infer_frequency : {pymor_result}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Works with Any Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 360_DAY Calendar ===\n",
      "Monthly data frequency: M\n",
      "Daily data frequency: D\n",
      "\n",
      "=== NOLEAP Calendar ===\n",
      "Monthly data frequency: M\n",
      "Daily data frequency: D\n",
      "\n",
      "=== STANDARD Calendar ===\n",
      "Monthly data frequency: M\n",
      "Daily data frequency: D\n"
     ]
    }
   ],
   "source": [
    "# Test different calendar types\n",
    "calendars_to_test = {\n",
    "    \"360_day\": cftime.Datetime360Day,\n",
    "    \"noleap\": cftime.DatetimeNoLeap,\n",
    "    \"standard\": cftime.DatetimeGregorian\n",
    "}\n",
    "\n",
    "for calendar_name, datetime_class in calendars_to_test.items():\n",
    "    print(f\"\\n=== {calendar_name.upper()} Calendar ===\")\n",
    "    \n",
    "    # Create monthly data\n",
    "    monthly_times = [\n",
    "        datetime_class(2000, 1, 15),\n",
    "        datetime_class(2000, 2, 15),\n",
    "        datetime_class(2000, 3, 15),\n",
    "        datetime_class(2000, 4, 15),\n",
    "        datetime_class(2000, 5, 15),\n",
    "    ]\n",
    "    \n",
    "    result = infer_frequency(monthly_times, calendar=calendar_name)\n",
    "    print(f\"Monthly data frequency: {result}\")\n",
    "    \n",
    "    # Create daily data\n",
    "    daily_times = [\n",
    "        datetime_class(2000, 1, 1),\n",
    "        datetime_class(2000, 1, 2),\n",
    "        datetime_class(2000, 1, 3),\n",
    "        datetime_class(2000, 1, 4),\n",
    "        datetime_class(2000, 1, 5),\n",
    "    ]\n",
    "    \n",
    "    result = infer_frequency(daily_times, calendar=calendar_name)\n",
    "    print(f\"Daily data frequency: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Rich Diagnostics\n",
    "\n",
    "when parameter `return_metadata=True` is set, `infer_frequency` returns `FrequencyResult` object instead of a simple string. This object contains additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "  → Frequency: M\n",
      "  → Delta (days): 27.0\n",
      "  → Step: 1\n",
      "  → Is exact: False\n",
      "  → Status: irregular\n"
     ]
    }
   ],
   "source": [
    "times = [\n",
    "    \"2000-01-01\",\n",
    "    \"2000-02-01\",\n",
    "    \"2000-02-28\",  #  <- 1 day offset \n",
    "    \"2000-04-01\",\n",
    "]\n",
    "\n",
    "result = infer_frequency(times, return_metadata=True, strict=True)\n",
    "print(\"Inference:\")\n",
    "print(f\"  → Frequency: {result.frequency}\")\n",
    "print(f\"  → Delta (days): {result.delta_days}\")\n",
    "print(f\"  → Step: {result.step}\")\n",
    "print(f\"  → Is exact: {result.is_exact}\")\n",
    "print(f\"  → Status: {result.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Testing Different Scenarios:\n",
      "\n",
      "PERFECT MONTHLY:\n",
      "data: DatetimeIndex(['2000-01-01', '2000-02-01', '2000-03-01', '2000-04-01',\n",
      "               '2000-05-01', '2000-06-01', '2000-07-01', '2000-08-01',\n",
      "               '2000-09-01', '2000-10-01', '2000-11-01', '2000-12-01'],\n",
      "              dtype='datetime64[ns]', freq='MS')\n",
      "Inference:\n",
      "    → Frequency: MS\n",
      "    → Delta: 31.00 days\n",
      "    → Status: valid\n",
      "    → Exact: Yes\n",
      "--------------------------------------------------\n",
      "\n",
      "MONTHLY WITH GAP:\n",
      "data: DatetimeIndex(['2000-01-01', '2000-02-01', '2000-04-01', '2000-05-01'], dtype='datetime64[ns]', freq=None)\n",
      "Inference:\n",
      "    → Frequency: M\n",
      "    → Delta: 30.00 days\n",
      "    → Status: irregular\n",
      "    → Exact: No\n",
      "--------------------------------------------------\n",
      "\n",
      "DAILY DATA:\n",
      "data: DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04',\n",
      "               '2000-01-05', '2000-01-06', '2000-01-07'],\n",
      "              dtype='datetime64[ns]', freq='D')\n",
      "Inference:\n",
      "    → Frequency: D\n",
      "    → Delta: 1.00 days\n",
      "    → Status: valid\n",
      "    → Exact: Yes\n",
      "--------------------------------------------------\n",
      "\n",
      "IRREGULAR DAILY:\n",
      "data: DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', freq=None)\n",
      "Inference:\n",
      "    → Frequency: D\n",
      "    → Delta: 1.00 days\n",
      "    → Status: missing_steps\n",
      "    → Exact: No\n",
      "--------------------------------------------------\n",
      "\n",
      "TOO SHORT:\n",
      "data: DatetimeIndex(['2000-01-01'], dtype='datetime64[ns]', freq=None)\n",
      "Inference:\n",
      "  → Could not infer frequency\n",
      "  → Status: too_short\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test different scenarios\n",
    "print(\"\\n📊 Testing Different Scenarios:\")\n",
    "scenarios = {\n",
    "    \"Perfect monthly\": pd.date_range(\"2000-01-01\", periods=12, freq=\"MS\"),\n",
    "    \"Monthly with gap\": pd.to_datetime([\"2000-01-01\", \"2000-02-01\", \"2000-04-01\", \"2000-05-01\"]),\n",
    "    \"Daily data\": pd.date_range(\"2000-01-01\", periods=7, freq=\"D\"),\n",
    "    \"Irregular daily\": pd.to_datetime([\"2000-01-01\", \"2000-01-02\", \"2000-01-04\", \"2000-01-05\"]),\n",
    "    \"Too short\": pd.to_datetime([\"2000-01-01\"]),\n",
    "}\n",
    "\n",
    "for scenario_name, times in scenarios.items():\n",
    "    print(f\"\\n{scenario_name.upper()}:\")\n",
    "    print(f\"data: {times}\")\n",
    "    result = infer_frequency(times, return_metadata=True, strict=True)\n",
    "    print(\"Inference:\")\n",
    "    if result.frequency:\n",
    "        print(f\"    → Frequency: {result.frequency}\")\n",
    "        if result.delta_days is not None:\n",
    "            print(f\"    → Delta: {result.delta_days:.2f} days\")\n",
    "        else:\n",
    "            print(f\"    → Delta: None\")\n",
    "        print(f\"    → Status: {result.status}\")\n",
    "        print(f\"    → Exact: {'Yes' if result.is_exact else 'No'}\")\n",
    "    else:\n",
    "        print(f\"  → Could not infer frequency\")\n",
    "        print(f\"  → Status: {result.status}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Handle data overlaps and duplicates\n",
    "\n",
    "`infer_frequency` can detect the underlying frequency of the data inspite of data overlaps or duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "DatetimeIndex(['2000-01-01', '2000-02-01', '2000-03-01'], dtype='datetime64[ns]', freq=None)\n",
      "Inference:\n",
      "  → Frequency: MS\n",
      "  → Status: valid\n",
      "  → Is exact: True\n"
     ]
    }
   ],
   "source": [
    "# Original monthly data\n",
    "data = pd.to_datetime([\"2000-01-01\", \"2000-02-01\", \"2000-03-01\"])\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "result = infer_frequency(data, return_metadata=True)\n",
    "print(\"Inference:\")\n",
    "print(f\"  → Frequency: {result.frequency}\")\n",
    "print(f\"  → Status: {result.status}\")\n",
    "print(f\"  → Is exact: {result.is_exact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicated data:\n",
      "['2000-01-01T00:00:00.000000000' '2000-02-01T00:00:00.000000000'\n",
      " '2000-03-01T00:00:00.000000000' '2000-01-01T00:00:00.000000000'\n",
      " '2000-02-01T00:00:00.000000000' '2000-03-01T00:00:00.000000000']\n",
      "Inference:\n",
      "  → Frequency: M\n",
      "  → Status: irregular\n",
      "  → Is exact: False\n"
     ]
    }
   ],
   "source": [
    "# Simulate concatenating the same file twice (common mistake!)\n",
    "duplicated_data = np.tile(data, 2)  # [Jan, Feb, Mar, Jan, Feb, Mar]\n",
    "print(\"\\nDuplicated data:\")\n",
    "print(f\"{duplicated_data}\")\n",
    "\n",
    "result = infer_frequency(duplicated_data, return_metadata=True)\n",
    "print(\"Inference:\")\n",
    "print(f\"  → Frequency: {result.frequency}\")\n",
    "print(f\"  → Status: {result.status}\")\n",
    "print(f\"  → Is exact: {result.is_exact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Understanding FrequencyResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE STATUS EXAMPLES ===\n",
      "\n",
      "Valid regular series:\n",
      "  Times: 12 points\n",
      "  → Frequency: MS\n",
      "  → Status: valid\n",
      "  → Is exact: True\n",
      "  → Median Δ: 31.0 days\n",
      "  ✅ Safe for resampling/analysis\n",
      "\n",
      "Missing steps:\n",
      "  Times: 4 points\n",
      "  Range: 2000-01-01 to 2000-05-01\n",
      "  → Frequency: M\n",
      "  → Status: irregular\n",
      "  → Is exact: False\n",
      "  → Median Δ: 30.0 days\n",
      "  ⚠️  Underlying frequency exists, but beware of inconsistencies\n",
      "\n",
      "Irregular spacing:\n",
      "  Times: 4 points\n",
      "  Range: 2000-01-01 to 2000-04-01\n",
      "  → Frequency: M\n",
      "  → Status: irregular\n",
      "  → Is exact: False\n",
      "  → Median Δ: 28.0 days\n",
      "  ⚠️  Underlying frequency exists, but beware of inconsistencies\n",
      "\n",
      "Too short:\n",
      "  Times: 1 points\n",
      "  → Frequency: None\n",
      "  → Status: too_short\n",
      "  → Is exact: False\n",
      "  ❌ Not enough points to determine frequency\n",
      "\n",
      "With duplicates:\n",
      "  Times: 4 points\n",
      "  Range: 2000-01-01 to 2000-03-01\n",
      "  → Frequency: M\n",
      "  → Status: missing_steps\n",
      "  → Is exact: False\n",
      "  → Median Δ: 29.0 days\n",
      "  ⚠️  Has gaps - consider filling before analysis\n"
     ]
    }
   ],
   "source": [
    "test_cases = {\n",
    "    \"Valid regular series\": pd.date_range(\"2000-01-01\", periods=12, freq=\"MS\"),\n",
    "    \"Missing steps\": pd.to_datetime([\"2000-01-01\", \"2000-02-01\", \"2000-04-01\", \"2000-05-01\"]),\n",
    "    \"Irregular spacing\": pd.to_datetime([\"2000-01-01\", \"2000-01-31\", \"2000-02-28\", \"2000-04-01\"]),\n",
    "    \"Too short\": pd.to_datetime([\"2000-01-01\"]),\n",
    "    \"With duplicates\": pd.to_datetime([\"2000-01-01\", \"2000-01-01\", \"2000-02-01\", \"2000-03-01\"]),\n",
    "}\n",
    "\n",
    "print(\"=== COMPREHENSIVE STATUS EXAMPLES ===\")\n",
    "for case_name, times in test_cases.items():\n",
    "    print(f\"\\n{case_name}:\")\n",
    "    result = infer_frequency(times, return_metadata=True, strict=True)\n",
    "    \n",
    "    print(f\"  Times: {len(times)} points\")\n",
    "    if len(times) > 1 and len(times) <= 4:\n",
    "        print(f\"  Range: {times[0].strftime('%Y-%m-%d')} to {times[-1].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    print(f\"  → Frequency: {result.frequency or 'None'}\")\n",
    "    print(f\"  → Status: {result.status}\")\n",
    "    print(f\"  → Is exact: {result.is_exact}\")\n",
    "    if result.delta_days:\n",
    "        print(f\"  → Median Δ: {result.delta_days:.1f} days\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if result.status == \"valid\" and result.is_exact:\n",
    "        print(\"  ✅ Safe for resampling/analysis\")\n",
    "    elif result.status == \"missing_steps\":\n",
    "        print(\"  ⚠️  Has gaps - consider filling before analysis\")\n",
    "    elif result.status == \"irregular\":\n",
    "        print(\"  ⚠️  Underlying frequency exists, but beware of inconsistencies\")\n",
    "    elif result.status == \"too_short\":\n",
    "        print(\"  ❌ Not enough points to determine frequency\")\n",
    "    else:\n",
    "        print(f\"  ❓ Status: {result.status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preventing Subtle Errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESAMPLING SAFETY CHECK ===\n",
      "\n",
      "Daily data:\n",
      "  Frequency: D\n",
      "  Status: valid\n",
      "  detla: 1.0\n",
      "  Safe for monthly resampling: ✅\n",
      "\n",
      "Monthly data:\n",
      "  Frequency: MS\n",
      "  Status: valid\n",
      "  Safe for annual resampling: ✅\n",
      "  Safe for daily resampling: ❌ (would be upsampling)\n",
      "\n",
      "⚠️  UPSAMPLING RISK:\n",
      "   Trying to resample monthly data (31.0 day intervals)\n",
      "   to daily frequency (1 day intervals) would create artificial data points!\n"
     ]
    }
   ],
   "source": [
    "# Create some sample climate data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Daily temperature data\n",
    "daily_times = pd.date_range(\"2000-01-01\", \"2000-12-31\", freq=\"D\")\n",
    "daily_temps = 15 + 10 * np.sin(2 * np.pi * np.arange(len(daily_times)) / 365.25) + np.random.normal(0, 2, len(daily_times))\n",
    "\n",
    "# Monthly temperature data (subset)\n",
    "monthly_times = pd.date_range(\"2000-01-01\", \"2000-12-01\", freq=\"MS\")\n",
    "monthly_temps = 15 + 10 * np.sin(2 * np.pi * np.arange(len(monthly_times)) / 12) + np.random.normal(0, 1, len(monthly_times))\n",
    "\n",
    "print(\"=== RESAMPLING SAFETY CHECK ===\")\n",
    "\n",
    "# Check daily data\n",
    "daily_result = infer_frequency(daily_times, return_metadata=True, strict=True)\n",
    "print(f\"\\nDaily data:\")\n",
    "print(f\"  Frequency: {daily_result.frequency}\")\n",
    "print(f\"  Status: {daily_result.status}\")\n",
    "print(f\"  detla: {daily_result.delta_days}\")\n",
    "print(f\"  Safe for monthly resampling: {'✅' if daily_result.delta_days and daily_result.delta_days < 30 else '❌'}\")\n",
    "\n",
    "# Check monthly data\n",
    "monthly_result = infer_frequency(monthly_times, return_metadata=True, strict=True)\n",
    "print(f\"\\nMonthly data:\")\n",
    "print(f\"  Frequency: {monthly_result.frequency}\")\n",
    "print(f\"  Status: {monthly_result.status}\")\n",
    "print(f\"  Safe for annual resampling: {'✅' if monthly_result.delta_days and monthly_result.delta_days < 365 else '❌'}\")\n",
    "print(f\"  Safe for daily resampling: {'❌ (would be upsampling)' if monthly_result.delta_days and monthly_result.delta_days > 1 else '✅'}\")\n",
    "\n",
    "# Demonstrate the risk of upsampling\n",
    "print(f\"\\n⚠️  UPSAMPLING RISK:\")\n",
    "if monthly_result.delta_days is not None:\n",
    "    print(f\"   Trying to resample monthly data ({monthly_result.delta_days:.1f} day intervals)\")\n",
    "    print(f\"   to daily frequency (1 day intervals) would create artificial data points!\")\n",
    "else:\n",
    "    print(f\"   Monthly data has unknown interval - check before resampling to daily!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### File Concatenation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FILE CONCATENATION ANALYSIS ===\n",
      "\n",
      "=== CHECK INDIVIDUAL FILES ===\n",
      "File 1: 182 time points\n",
      "  Range: 2000-01-01 to 2000-06-30\n",
      "  Frequency: D\n",
      "  Status: valid\n",
      "  Is exact: True\n",
      "\n",
      "File 2: 183 time points\n",
      "  Range: 2000-07-01 to 2000-12-31\n",
      "  Frequency: D\n",
      "  Status: missing_steps\n",
      "  Is exact: False\n",
      "\n",
      "File 3: 212 time points\n",
      "  Range: 2000-12-01 to 2001-06-30\n",
      "  Frequency: D\n",
      "  Status: valid\n",
      "  Is exact: True\n",
      "\n",
      "\n",
      "=== COMBINED DATASET ===\n",
      "Total time points: 546\n",
      "Range: 2000-01-01 to 2001-06-30\n",
      "Frequency: D\n",
      "Status: missing_steps\n",
      "Is exact: False\n",
      "\n",
      "⚠️  ISSUES DETECTED:\n",
      "   - Missing time steps detected\n",
      "   - Recommend investigating data before analysis\n"
     ]
    }
   ],
   "source": [
    "# Simulate concatenating multiple NetCDF files with potential issues\n",
    "\n",
    "# File 1: Jan-Jun 2000\n",
    "file1_times = pd.date_range(\"2000-01-01\", \"2000-06-30\", freq=\"D\")\n",
    "\n",
    "# File 2: Jul-Dec 2000 (but with a gap - missing July 15)\n",
    "file2_start = pd.date_range(\"2000-07-01\", \"2000-07-14\", freq=\"D\")\n",
    "file2_end = pd.date_range(\"2000-07-16\", \"2000-12-31\", freq=\"D\")\n",
    "file2_times = file2_start.union(file2_end)\n",
    "\n",
    "# File 3: Overlap with file 2 (Dec 2000 repeated)\n",
    "file3_times = pd.date_range(\"2000-12-01\", \"2001-06-30\", freq=\"D\")\n",
    "\n",
    "\n",
    "print(\"=== FILE CONCATENATION ANALYSIS ===\")\n",
    "\n",
    "print(\"\\n=== CHECK INDIVIDUAL FILES ===\")\n",
    "\n",
    "for i, times in enumerate([file1_times, file2_times, file3_times], 1):\n",
    "    result = infer_frequency(times, return_metadata=True, strict=True)\n",
    "    print(f\"File {i}: {len(times)} time points\")\n",
    "    print(f\"  Range: {times[0].strftime('%Y-%m-%d')} to {times[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Frequency: {result.frequency}\")\n",
    "    print(f\"  Status: {result.status}\")\n",
    "    print(f\"  Is exact: {result.is_exact}\")\n",
    "    print()\n",
    "\n",
    "# Concatenate all files\n",
    "combined_times = file1_times.union(file2_times).union(file3_times)\n",
    "combined_result = infer_frequency(combined_times, return_metadata=True, strict=True)\n",
    "\n",
    "print(\"\\n=== COMBINED DATASET ===\")\n",
    "print(f\"Total time points: {len(combined_times)}\")\n",
    "print(f\"Range: {combined_times[0].strftime('%Y-%m-%d')} to {combined_times[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Frequency: {combined_result.frequency}\")\n",
    "print(f\"Status: {combined_result.status}\")\n",
    "print(f\"Is exact: {combined_result.is_exact}\")\n",
    "\n",
    "\n",
    "if combined_result.status != \"valid\" or not combined_result.is_exact:\n",
    "    print(f\"\\n⚠️  ISSUES DETECTED:\")\n",
    "    if \"missing\" in combined_result.status:\n",
    "        print(f\"   - Missing time steps detected\")\n",
    "    if \"irregular\" in combined_result.status:\n",
    "        print(f\"   - Irregular spacing or duplicates detected\")\n",
    "    print(f\"   - Recommend investigating data before analysis\")\n",
    "else:\n",
    "    print(f\"\\n✅ Combined dataset looks good for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary and Takeaways\n",
    "\n",
    "🎯 KEY TAKEAWAYS:\n",
    "\n",
    "1. **Resilient to irregularities**: Our infer_frequency handles gaps, duplicates, \n",
    "   and non-standard calendars that break standard tools.\n",
    "\n",
    "2. **Transparent diagnostics**: Instead of silent failures, you get detailed \n",
    "   information about what was found and why.\n",
    "\n",
    "3. **Tailored for climate data**: Built specifically for the messy realities \n",
    "   of climate model output and observational data.\n",
    "\n",
    "4. **Prevents subtle errors**: Programmatically detect issues before they \n",
    "   propagate into your analysis pipeline.\n",
    "\n",
    "5. **Easy integration**: Works with xarray, pandas, and cftime objects \n",
    "   out of the box.\n",
    "\n",
    "The pymor.core.infer_freq module turns guesswork into a reliable, automated \n",
    "process—so you can spend less time debugging and more time doing science.\n",
    "\n",
    "Stop guessing. Start inferring—smarter.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Repository\n",
    "\n",
    "- GitHub: [esm-tools/pymor](https://github.com/esm-tools/pymor)\n",
    "- PyPI: [py-cmor](https://pypi.org/project/py-cmor/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "This work was developed by the High Performance Computing and Data Processing\n",
    "group at the Alfred Wegener Institute for Polar and Marine Research (AWI),\n",
    "Bremerhaven, Germany.\n",
    "\n",
    "- Pavan Kumar Siligam (AWI) - [ORCID: 0009-0003-8054-7021](https://orcid.org/0009-0003-8054-7021)\n",
    "- Paul Gierz (AWI) - [ORCID: 0000-0002-4512-087X](https://orcid.org/0000-0002-4512-087X)\n",
    "- Miguel Andrés-Martínez (AWI) - [ORCID: 0000-0002-1525-5546](https://orcid.org/0000-0002-1525-5546)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
